{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the pandas package, then use the \"read_csv\" function to read\n",
    "# the labeled training data\n",
    "import pandas as pd       \n",
    "tontozona = pd.read_csv(\"E:/ASU Energy Leadership Informatics/neptune files/Maintenance Requests/Campus CSV Files/Topic Modeling/TONTOZONA.csv\", header=0, encoding=\"cp1252\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import BeautifulSoup into workspace\n",
    "from bs4 import BeautifulSoup             \n",
    "import re\n",
    "import nltk\n",
    "# Download text data sets, including stop words\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to reuse the code\n",
    "def maintenance(tma):\n",
    "    # Function to convert a raw action request to a string of words\n",
    "    # The input is a single string, and \n",
    "    # the output is a single string (a processed action request)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    text = BeautifulSoup(tma,\"html.parser\").get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-z A-Z]\", \" \", text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the number of requests based on the dataframe column size\n",
    "num_requests = tontozona[\"ActionRequested\"].size\n",
    "\n",
    "# Initialize an empty list to hold the clean requests\n",
    "clean_train_requests = []\n",
    "\n",
    "# Loop over each request; create an index i that goes from 0 to the length\n",
    "# of the maintenance request list \n",
    "for i in range( 0, num_requests ):\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "    # clean requests\n",
    "    clean_train_requests.append(maintenance( tontozona[\"ActionRequested\"][i] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in tontozona[\"ActionRequested\"]]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the term dictionary of our corpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(doc_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Running and Training LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=6, id2word = dictionary, passes=100, alpha=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.285*\"furnacebiannual\" + 0.267*\"equipmentbiannual\" + 0.267*\"dx\" + 0.002*\"water\" + 0.002*\"heaterannual\" + 0.002*\"walkin\"'), (1, '0.187*\"walkin\" + 0.094*\"freezerbiannual\" + 0.094*\"coolerbiannual\" + 0.037*\"contact\" + 0.037*\"compressorquarterly\" + 0.025*\"tom\"'), (2, '0.220*\"cooler\" + 0.220*\"biannual\" + 0.220*\"evaporative\" + 0.037*\"fountainannual\" + 0.037*\"drinking\" + 0.007*\"attachment\"'), (3, '0.292*\"air\" + 0.185*\"handlerbiannual\" + 0.063*\"compressorbiannual\" + 0.018*\"replaced\" + 0.018*\"hall\" + 0.018*\"adjust\"'), (4, '0.280*\"water\" + 0.274*\"heaterannual\" + 0.046*\"exhaust\" + 0.046*\"fanbiannual\" + 0.035*\"electrical\" + 0.035*\"heaterbiannual\"'), (5, '0.226*\"fire\" + 0.220*\"inspectionquarterly\" + 0.220*\"pump\" + 0.019*\"camp\" + 0.013*\"multiple\" + 0.013*\"reconstruction\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=6, num_words=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\pyLDAvis\\_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n"
     ]
    }
   ],
   "source": [
    "vis1 =  pyLDAvis.gensim.prepare(ldamodel, doc_term_matrix, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el328821309702851841786493799\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el328821309702851841786493799_data = {\"mdsDat\": {\"Freq\": [24.160496745487716, 22.483866985023162, 22.478265684735906, 13.318115025146154, 10.40664919328585, 7.152606366321212], \"cluster\": [1, 1, 1, 1, 1, 1], \"topics\": [1, 2, 3, 4, 5, 6], \"x\": [-0.3128074033939356, 0.1935327482902927, 0.13368847896841154, -0.010514999870774437, -0.019120739540164362, 0.015221915546170525], \"y\": [-0.05295870149758968, -0.2436537227208845, 0.23916205999658288, 0.15169480551369033, -0.04791535311232707, -0.04632908817947218]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\"], \"Freq\": [44.0, 43.0, 25.0, 33.0, 23.0, 23.0, 32.0, 32.0, 32.0, 32.0, 33.0, 14.0, 13.0, 9.0, 7.0, 7.0, 3.0, 7.0, 7.0, 7.0, 6.0, 6.0, 6.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 43.616472504188465, 42.72935780788803, 7.244770012685145, 7.244770012685145, 5.470540616533261, 3.696311231034278, 3.696311231034278, 3.696311231034278, 5.470540616533261, 1.0349671456838692, 1.0349671456838692, 1.0349671456838692, 1.0349671456838692, 1.0349671456838692, 1.0349671456838692, 1.0349671456838692, 1.0349671456838692, 1.0349671456838692, 1.0349671456838692, 1.0349671456838692, 1.034584324349537, 1.034584324349537, 1.034584324349537, 1.034584324349537, 1.0349671456838692, 1.0349671456838692, 1.0349671456838692, 1.0349671456838692, 0.14785244938340988, 0.14785244938340988, 0.14810623859244268, 0.14785245648534756, 32.731921342773155, 31.851487014268834, 31.851487014268834, 1.9081536036772266, 1.9081536036749123, 1.9081536036675004, 1.9081536036629692, 1.9081536036467326, 1.9081536036411175, 1.9081536036339737, 1.0274673250351305, 1.0274673250218511, 1.027467325020472, 1.0274673250169157, 1.0274673250137665, 1.0274673250126694, 1.027467325009818, 1.027467325007305, 1.0274673250029802, 1.0274673249982285, 1.0274673249950814, 1.0274673249891446, 1.0274673249885364, 1.9081536036556526, 1.9081536036297146, 2.788839882116856, 1.908153603465023, 0.1467810464411203, 0.1467810464411203, 0.1467810464411203, 0.14716109368611113, 0.14716109368611113, 0.14716109368611113, 0.14716109368611113, 31.843830479437504, 31.843830479437504, 31.843830479437504, 5.429593212626921, 5.429593212626921, 1.0272203395243409, 1.0272203395243409, 1.0272203395243409, 1.0272203395243409, 1.0272203395243409, 1.0272203395243409, 1.0272203395243409, 1.0272203395243409, 1.0272203395243409, 1.0272203395243409, 1.0272203395243409, 1.0272203395243409, 1.0272203395243409, 1.0272203395243409, 1.0272203395243409, 1.0272203395243409, 1.0272203395243409, 1.0272203395243409, 1.0272203395243409, 1.0272203395243409, 1.0272203395243409, 1.0272203395243409, 1.0272203395243409, 1.027220339793817, 1.027220339788707, 24.524027381517424, 22.89812503126497, 22.89812503126497, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549188692682512, 0.13549188692682512, 0.1354918739104053, 0.13549187065630036, 0.1354918674021954, 0.1354918674021954, 0.1354918674021954, 0.1354918674021954, 0.1354918674021954, 0.1354918674021954, 0.1354918674021954, 0.1354918674021954, 0.1354918674021954, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 0.13549186414809045, 12.521393234892907, 6.325239881639722, 6.325239881639722, 2.4526440393442863, 2.4503681999487394, 1.6781248690250377, 1.6781248690250377, 1.6781248690250377, 1.6781248616897249, 1.6781248616897249, 0.9036056987057896, 0.9036056987057896, 0.9036056987057896, 0.9036056987057896, 0.9036056987057896, 0.9036056987057896, 0.9036056987057896, 0.9036056987057896, 0.9036056987057896, 0.9036056987057896, 0.9036056987057896, 0.9036056987057896, 0.9036056987057896, 0.9036056987057896, 0.9036056987057896, 0.9036056987057896, 1.6781248616897249, 0.1290865283865414, 0.1290865283865414, 0.1290865283865414, 0.9036057142071305, 0.1290865283865414, 0.1290865283865414, 0.12925481697086758, 0.12908654698815028, 13.464887762703686, 8.547373473819839, 2.9271826966677303, 0.8196111566417309, 0.8196111566417309, 0.8196111566417309, 0.8196111566417309, 0.8196111566417309, 0.8196111566417306, 0.8196111566417306, 0.8196111566417306, 0.8196111566417306, 0.8196111566417306, 0.8196111566417306, 0.11708730809167583, 0.11708730809167583, 0.11708730809167583, 0.11708730809167583, 0.11708730809167583, 0.11708730809167583, 0.11708730809167583, 0.11708730809167583, 0.11708730809167583, 0.11708730809167583, 0.11708730809167583, 0.11708730809167583, 0.11708730809167583, 0.11708730809167583, 0.11708730809167583, 0.11708730809167583, 0.11915159152853066, 0.11708730809167583, 0.11708732777625792, 0.11708732496417476, 0.11708732496417476, 0.11708732496417476, 0.11708731090375897, 0.11708731090375897, 0.11708731090375897, 0.11708731090375897, 0.11708730809167583, 0.11708730809167583, 0.11708730809167583, 0.11708730809167583, 0.11708730809167583, 0.11708730809167583, 0.11708730809167583], \"Term\": [\"water\", \"heaterannual\", \"furnacebiannual\", \"fire\", \"equipmentbiannual\", \"dx\", \"inspectionquarterly\", \"biannual\", \"cooler\", \"evaporative\", \"pump\", \"air\", \"walkin\", \"handlerbiannual\", \"freezerbiannual\", \"coolerbiannual\", \"compressorbiannual\", \"fanbiannual\", \"exhaust\", \"heaterbiannual\", \"fountainannual\", \"drinking\", \"electrical\", \"contact\", \"compressorquarterly\", \"fraker\", \"4802206588\", \"tom\", \"hanging\", \"gas\", \"water\", \"heaterannual\", \"exhaust\", \"fanbiannual\", \"electrical\", \"ice\", \"machine\", \"cubebiannual\", \"heaterbiannual\", \"tmp\", \"install\", \"4\", \"gallon\", \"drain\", \"line\", \"tank\", \"300000\", \"est\", \"storage\", \"152547\", \"hydrant\", \"2\", \"replace\", \"broken\", \"provide\", \"estimate\", \"plumbing\", \"please\", \"replaced\", \"lamp\", \"fire\", \"freezerbiannual\", \"fire\", \"inspectionquarterly\", \"pump\", \"multiple\", \"reconstruction\", \"issue\", \"safety\", \"regular\", \"need\", \"addressed\", \"01142015\", \"42c763o\", \"bruce\", \"nevel\", \"sam\", \"clint\", \"weather\", \"permitting\", \"plan\", \"lord\", \"going\", \"see\", \"wheeler\", \"tontozona\", \"maintenance\", \"camp\", \"attachment\", \"replaced\", \"lamp\", \"lens\", \"replace\", \"2\", \"broken\", \"hydrant\", \"biannual\", \"cooler\", \"evaporative\", \"fountainannual\", \"drinking\", \"pay\", \"want\", \"good\", \"request\", \"labeling\", \"equipment\", \"audit\", \"complete\", \"42c72cc\", \"outstanding\", \"invoice\", \"asbestos\", \"testing\", \"use\", \"following\", \"account\", \"bill\", \"against\", \"gn7\", \"1165\", \"90\", \"morning\", \"please\", \"attachment\", \"camp\", \"furnacebiannual\", \"equipmentbiannual\", \"dx\", \"lamp\", \"lens\", \"adjust\", \"hall\", \"replaced\", \"preventive\", \"name\", \"42ablnj\", \"tomtontocreekcamporg\", \"email\", \"number\", \"phone\", \"thank\", \"someone\", \"major\", \"today\", \"call\", \"shop\", \"house\", \"leak\", \"following\", \"account\", \"bill\", \"against\", \"testing\", \"gn7\", \"42c72cc\", \"water\", \"heaterannual\", \"walkin\", \"coolerbiannual\", \"fountainannual\", \"biannual\", \"cooler\", \"evaporative\", \"freezerbiannual\", \"fire\", \"drinking\", \"pump\", \"inspectionquarterly\", \"air\", \"asbestos\", \"please\", \"use\", \"outstanding\", \"1165\", \"90\", \"attachment\", \"walkin\", \"coolerbiannual\", \"freezerbiannual\", \"contact\", \"compressorquarterly\", \"fraker\", \"4802206588\", \"tom\", \"hanging\", \"gas\", \"shop\", \"tomtontocreekcamporg\", \"email\", \"number\", \"42ablnj\", \"call\", \"thank\", \"today\", \"house\", \"leak\", \"major\", \"phone\", \"name\", \"someone\", \"plumbing\", \"please\", \"heaterbiannual\", \"preventive\", \"replaced\", \"hall\", \"pump\", \"lens\", \"adjust\", \"air\", \"biannual\", \"air\", \"handlerbiannual\", \"compressorbiannual\", \"lamp\", \"lens\", \"adjust\", \"hall\", \"replaced\", \"preventive\", \"estimate\", \"provide\", \"maintenance\", \"tontozona\", \"camp\", \"house\", \"thank\", \"number\", \"someone\", \"major\", \"email\", \"today\", \"name\", \"leak\", \"tomtontocreekcamporg\", \"42ablnj\", \"call\", \"shop\", \"phone\", \"testing\", \"gn7\", \"compressorquarterly\", \"account\", \"furnacebiannual\", \"inspectionquarterly\", \"pump\", \"fire\", \"water\", \"heaterannual\", \"coolerbiannual\", \"walkin\", \"need\", \"outstanding\", \"equipmentbiannual\", \"plan\", \"nevel\", \"good\", \"morning\"], \"Total\": [44.0, 43.0, 25.0, 33.0, 23.0, 23.0, 32.0, 32.0, 32.0, 32.0, 33.0, 14.0, 13.0, 9.0, 7.0, 7.0, 3.0, 7.0, 7.0, 7.0, 6.0, 6.0, 6.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 44.291665074032395, 43.40455037773196, 7.9199625295922385, 7.9199625295922385, 6.145733136963896, 4.371503743991165, 4.371503743991165, 4.371503743991165, 7.694771478605873, 1.7101596555404888, 1.7101596555404888, 1.7101596555404888, 1.7101596555404888, 1.7101596555404888, 1.7101596555404888, 1.7101596555404888, 1.7101596555404888, 1.7101596555404888, 1.7101596555404888, 1.7101596555404888, 1.7101568814511474, 1.7101568814511474, 1.7101568814511474, 1.7101568814511474, 2.4126835040905434, 2.4126835040905434, 2.4846788258597368, 3.365153402594886, 1.5255688077900844, 1.5255688077900844, 33.40843910155537, 7.0191983228492525, 33.40843910155537, 32.52775098384202, 33.30227015416127, 2.5844175165334584, 2.5844175165334584, 2.584417516533457, 2.5844175165334553, 2.5844175165334518, 2.5844175165334504, 2.5844175165334486, 1.7037312378867389, 1.7037312378867355, 1.7037312378867353, 1.7037312378867342, 1.7037312378867335, 1.7037312378867333, 1.7037312378867326, 1.703731237886732, 1.7037312378867313, 1.7037312378867298, 1.7037312378867293, 1.7037312378867278, 1.7037312378867273, 3.286941365083509, 3.2869413650835018, 5.048102220465336, 3.4648920932685576, 1.5255688077900844, 1.5255688077900844, 1.5255688077900844, 1.7101568814511474, 1.7101568814511474, 1.7101568814511474, 1.7101568814511474, 32.520129718895475, 32.520129718895475, 32.520129718895475, 6.105892418957368, 6.105892418957368, 1.703519535975179, 1.703519535975179, 1.703519535975179, 1.703519535975179, 1.703519535975179, 1.703519535975179, 1.703519535975179, 1.703519535975179, 1.703519535975179, 1.703519535975179, 1.703519535975179, 1.703519535975179, 1.703519535975179, 1.703519535975179, 1.703519535975179, 1.703519535975179, 1.703519535975179, 1.703519535975179, 1.703519535975179, 1.703519535975179, 1.703519535975179, 1.703519535975179, 3.365153402594886, 3.4648920932685576, 5.048102220465336, 25.21158050291945, 23.58567813650596, 23.58567813650596, 1.5255688077900844, 1.5255688077900844, 1.5255688077900844, 1.5255688077900844, 1.5255688077900844, 1.5255688077900842, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.703519535975179, 1.703519535975179, 1.703519535975179, 1.703519535975179, 1.703519535975179, 1.703519535975179, 1.703519535975179, 44.291665074032395, 43.40455037773196, 13.21535168542273, 7.019198321813503, 6.105892418957368, 32.520129718895475, 32.520129718895475, 32.520129718895475, 7.0191983228492525, 33.40843910155537, 6.105892418957368, 33.30227015416127, 32.52775098384202, 14.171013730687184, 1.703519535975179, 3.365153402594886, 1.703519535975179, 1.703519535975179, 1.703519535975179, 1.703519535975179, 3.4648920932685576, 13.21535168542273, 7.019198321813503, 7.0191983228492525, 3.146602470197774, 3.1463909213152883, 2.372083299878526, 2.372083299878526, 2.372083299878526, 2.372083300882008, 2.372083300882008, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 2.4846788258597368, 3.365153402594886, 7.694771478605873, 1.5255688077900842, 1.5255688077900844, 1.5255688077900844, 33.30227015416127, 1.5255688077900844, 1.5255688077900844, 14.171013730687184, 32.520129718895475, 14.171013730687184, 9.253331145267982, 3.6331403548914425, 1.5255688077900844, 1.5255688077900844, 1.5255688077900844, 1.5255688077900844, 1.5255688077900844, 1.5255688077900842, 2.4126835040905434, 2.4126835040905434, 3.2869413650835018, 3.286941365083509, 5.048102220465336, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.5975641295592777, 1.703519535975179, 1.703519535975179, 3.1463909213152883, 1.703519535975179, 25.21158050291945, 32.52775098384202, 33.30227015416127, 33.40843910155537, 44.291665074032395, 43.40455037773196, 7.019198321813503, 13.21535168542273, 2.5844175165334504, 1.703519535975179, 23.58567813650596, 1.7037312378867313, 1.7037312378867342, 1.703519535975179, 1.703519535975179], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.4051, 1.4048, 1.3313, 1.3313, 1.3041, 1.2527, 1.2527, 1.2527, 1.0793, 0.9182, 0.9182, 0.9182, 0.9182, 0.9182, 0.9182, 0.9182, 0.9182, 0.9182, 0.9182, 0.9182, 0.9179, 0.9179, 0.9179, 0.9179, 0.5741, 0.5741, 0.5447, 0.2413, -0.9135, -0.9135, -3.9982, -2.4397, 1.4719, 1.4714, 1.4478, 1.189, 1.189, 1.189, 1.189, 1.189, 1.189, 1.189, 0.9866, 0.9866, 0.9866, 0.9866, 0.9866, 0.9866, 0.9866, 0.9866, 0.9866, 0.9866, 0.9866, 0.9866, 0.9866, 0.9486, 0.9486, 0.899, 0.8958, -0.8488, -0.8488, -0.8488, -0.9604, -0.9604, -0.9604, -0.9604, 1.4716, 1.4716, 1.4716, 1.3752, 1.3752, 0.9868, 0.9868, 0.9868, 0.9868, 0.9868, 0.9868, 0.9868, 0.9868, 0.9868, 0.9868, 0.9868, 0.9868, 0.9868, 0.9868, 0.9868, 0.9868, 0.9868, 0.9868, 0.9868, 0.9868, 0.9868, 0.9868, 0.306, 0.2768, -0.0995, 1.9884, 1.9865, 1.9865, -0.4052, -0.4052, -0.4052, -0.4052, -0.4052, -0.4052, -0.4513, -0.4513, -0.4513, -0.4513, -0.4513, -0.4513, -0.4513, -0.4513, -0.4513, -0.4513, -0.4513, -0.4513, -0.4513, -0.4513, -0.5155, -0.5155, -0.5155, -0.5155, -0.5155, -0.5155, -0.5155, -3.7736, -3.7534, -2.5642, -1.9314, -1.7921, -3.4647, -3.4647, -3.4647, -1.9314, -3.4916, -1.7921, -3.4884, -3.4649, -2.634, -0.5155, -1.1963, -0.5155, -0.5155, -0.5155, -0.5155, -1.2255, 2.2088, 2.1586, 2.1586, 2.0136, 2.0127, 1.9166, 1.9166, 1.9166, 1.9166, 1.9166, 1.6929, 1.6929, 1.6929, 1.6929, 1.6929, 1.6929, 1.6929, 1.6929, 1.6929, 1.6929, 1.6929, 1.6929, 1.6929, 1.6929, 1.2512, 0.9479, 0.7399, -0.2069, -0.2069, -0.2069, -1.3443, -0.2069, -0.2069, -2.4344, -3.2664, 2.5866, 2.5583, 2.4216, 2.0164, 2.0164, 2.0164, 2.0164, 2.0164, 2.0164, 1.558, 1.558, 1.2488, 1.2488, 0.8198, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, 0.0244, -0.0398, -0.0398, -0.6359, -0.0398, -2.7344, -2.9892, -3.0128, -3.016, -3.2979, -3.2777, -1.4558, -2.0885, -0.4566, -0.0398, -2.6678, -0.04, -0.04, -0.0398, -0.0398], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -1.2734, -1.2939, -3.0685, -3.0685, -3.3494, -3.7415, -3.7415, -3.7415, -3.3494, -5.0144, -5.0144, -5.0144, -5.0144, -5.0144, -5.0144, -5.0144, -5.0144, -5.0144, -5.0144, -5.0144, -5.0148, -5.0148, -5.0148, -5.0148, -5.0144, -5.0144, -5.0144, -5.0144, -6.9603, -6.9603, -6.9586, -6.9603, -1.4885, -1.5158, -1.5158, -4.3307, -4.3307, -4.3307, -4.3307, -4.3307, -4.3307, -4.3307, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -4.3307, -4.3307, -3.9513, -4.3307, -6.8957, -6.8957, -6.8957, -6.8931, -6.8931, -6.8931, -6.8931, -1.5158, -1.5158, -1.5158, -3.2848, -3.2848, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -4.9498, -1.2536, -1.3222, -1.3222, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -6.452, -1.6791, -2.362, -2.362, -3.3094, -3.3103, -3.6888, -3.6888, -3.6888, -3.6888, -3.6888, -4.3079, -4.3079, -4.3079, -4.3079, -4.3079, -4.3079, -4.3079, -4.3079, -4.3079, -4.3079, -4.3079, -4.3079, -4.3079, -4.3079, -4.3079, -4.3079, -3.6888, -6.2538, -6.2538, -6.2538, -4.3079, -6.2538, -6.2538, -6.2525, -6.2538, -1.2315, -1.6859, -2.7575, -4.0305, -4.0305, -4.0305, -4.0305, -4.0305, -4.0305, -4.0305, -4.0305, -4.0305, -4.0305, -4.0305, -5.9764, -5.9764, -5.9764, -5.9764, -5.9764, -5.9764, -5.9764, -5.9764, -5.9764, -5.9764, -5.9764, -5.9764, -5.9764, -5.9764, -5.9764, -5.9764, -5.9589, -5.9764, -5.9764, -5.9764, -5.9764, -5.9764, -5.9764, -5.9764, -5.9764, -5.9764, -5.9764, -5.9764, -5.9764, -5.9764, -5.9764, -5.9764, -5.9764]}, \"token.table\": {\"Topic\": [2, 3, 1, 1, 1, 1, 5, 3, 2, 5, 3, 3, 2, 6, 3, 6, 3, 2, 3, 3, 3, 3, 1, 2, 5, 2, 3, 6, 2, 3, 6, 5, 5, 3, 5, 1, 1, 3, 4, 1, 5, 3, 4, 1, 1, 6, 3, 1, 1, 2, 3, 3, 5, 5, 4, 1, 5, 3, 2, 3, 6, 6, 5, 1, 1, 5, 5, 1, 1, 2, 1, 3, 2, 3, 6, 5, 6, 1, 2, 1, 2, 6, 5, 3, 2, 5, 2, 2, 5, 3, 3, 2, 5, 2, 1, 3, 5, 1, 5, 6, 1, 6, 2, 5, 2, 2, 1, 6, 3, 2, 2, 2, 5, 5, 1, 1, 3, 5, 1, 5, 5, 5, 2, 6, 3, 5, 3, 1, 2, 2], \"Freq\": [0.5869470358719092, 0.5870199776884569, 0.5847407268439824, 0.5847416753669135, 0.5847407268439824, 0.5847407268439824, 0.6259529626994513, 0.5870199776884569, 0.5869470358719103, 0.8431407109954443, 0.5870199776884569, 0.5870199776884569, 0.7738687681867502, 0.6554932133468202, 0.5870199776884569, 0.9173655637527635, 0.5870199776884569, 0.577218552891016, 0.288609276445508, 0.5870199776884569, 0.9840059150012166, 0.5870199776884569, 0.5847416753669135, 0.5869470358719104, 0.6259529626994513, 0.5942827361612854, 0.19809424538709514, 0.19809424538709514, 0.5869470358719111, 0.5870199776884569, 0.825731930769198, 0.6356489228502917, 0.6356061876078974, 0.9840059150012166, 0.8547984719784667, 0.9150169447980424, 0.5847407268439824, 0.818881116292873, 0.9751680603323657, 0.8135725858201014, 0.6259529626994513, 0.5870199776884569, 0.9751680603323657, 0.5847407268439824, 0.4144762453527646, 0.4144762453527646, 0.9840059150012166, 0.8838425653966316, 0.8838425653966316, 0.9877743734056597, 0.5870199776884569, 0.818881116292873, 0.8431407109954443, 0.854798471852333, 0.9916078048778041, 0.5847407268439824, 0.8431407106387634, 0.5870199776884569, 0.5869470358719125, 0.5870199776884569, 0.6554932133468202, 0.9726227083748611, 0.8431407106387634, 0.9906795399511958, 0.6497918767180715, 0.2599167506872286, 0.6259529626994513, 0.5847416753669135, 0.9150169447980424, 0.9837753620253618, 0.5847407268439824, 0.5870199776884569, 0.7738687681867477, 0.5870199776884569, 0.6554932133468202, 0.6259529626994513, 0.6554932133468202, 0.5847407268439824, 0.5869470358719123, 0.9150169447980424, 0.6084684142058591, 0.30423420710292953, 0.6259529626994513, 0.5870199776884569, 0.7738687681867472, 0.6259529626994513, 0.7738687681867497, 0.5869470358719108, 0.6259529626994513, 0.5870199776884569, 0.5870199776884569, 0.5869470358719115, 0.6259529626994513, 0.5869470358719118, 0.2971632732192521, 0.2971632732192521, 0.2971632732192521, 0.4024665037558666, 0.4024665037558666, 0.6554932133468203, 0.4144762453527646, 0.4144762453527646, 0.9608954540296243, 0.03002798293842576, 0.7738687681867472, 0.7738687681867492, 0.5847416753669135, 0.6554932133468202, 0.5870199776884569, 0.7738687681867482, 0.586947035871911, 0.586947035871913, 0.6259529626994513, 0.6259529626994513, 0.5847407268439824, 0.5847407268439824, 0.5870199776884569, 0.6259529626994513, 0.5847407268439824, 0.6259529626994513, 0.8431407109954443, 0.6259529626994513, 0.6084684142058577, 0.30423420710292887, 0.5870199776884569, 0.9837044302301636, 0.5870199776884569, 0.9934148993146931, 0.5869470358719113, 0.5869470358719132], \"Term\": [\"01142015\", \"1165\", \"152547\", \"2\", \"300000\", \"4\", \"42ablnj\", \"42c72cc\", \"42c763o\", \"4802206588\", \"90\", \"account\", \"addressed\", \"adjust\", \"against\", \"air\", \"asbestos\", \"attachment\", \"attachment\", \"audit\", \"biannual\", \"bill\", \"broken\", \"bruce\", \"call\", \"camp\", \"camp\", \"camp\", \"clint\", \"complete\", \"compressorbiannual\", \"compressorquarterly\", \"contact\", \"cooler\", \"coolerbiannual\", \"cubebiannual\", \"drain\", \"drinking\", \"dx\", \"electrical\", \"email\", \"equipment\", \"equipmentbiannual\", \"est\", \"estimate\", \"estimate\", \"evaporative\", \"exhaust\", \"fanbiannual\", \"fire\", \"following\", \"fountainannual\", \"fraker\", \"freezerbiannual\", \"furnacebiannual\", \"gallon\", \"gas\", \"gn7\", \"going\", \"good\", \"hall\", \"handlerbiannual\", \"hanging\", \"heaterannual\", \"heaterbiannual\", \"heaterbiannual\", \"house\", \"hydrant\", \"ice\", \"inspectionquarterly\", \"install\", \"invoice\", \"issue\", \"labeling\", \"lamp\", \"leak\", \"lens\", \"line\", \"lord\", \"machine\", \"maintenance\", \"maintenance\", \"major\", \"morning\", \"multiple\", \"name\", \"need\", \"nevel\", \"number\", \"outstanding\", \"pay\", \"permitting\", \"phone\", \"plan\", \"please\", \"please\", \"please\", \"plumbing\", \"plumbing\", \"preventive\", \"provide\", \"provide\", \"pump\", \"pump\", \"reconstruction\", \"regular\", \"replace\", \"replaced\", \"request\", \"safety\", \"sam\", \"see\", \"shop\", \"someone\", \"storage\", \"tank\", \"testing\", \"thank\", \"tmp\", \"today\", \"tom\", \"tomtontocreekcamporg\", \"tontozona\", \"tontozona\", \"use\", \"walkin\", \"want\", \"water\", \"weather\", \"wheeler\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [5, 6, 3, 1, 2, 4]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el328821309702851841786493799\", ldavis_el328821309702851841786493799_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el328821309702851841786493799\", ldavis_el328821309702851841786493799_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el328821309702851841786493799\", ldavis_el328821309702851841786493799_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.display(vis1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###                                                                                                         Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to train Word2Vec it is better not to remove stop words because the algorithm relies on the broader context of the sentence in order to produce high-quality word vectors. \n",
    "#Function to reuse the code\n",
    "def maintenance( tma, remove_stopwords=False):\n",
    "    # Function to convert a raw action request to a string of words\n",
    "    # The input is a single string, and \n",
    "    # the output is a single string (a processed action request)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    text = BeautifulSoup(tma,\"html.parser\").get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]                  \n",
    "    \n",
    "    # 5. Return the result.\n",
    "    return( words)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download the punkt tokenizer for sentence splitting\n",
    "import nltk.data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to split an action request into parsed sentences\n",
    "def request_to_sentences(tma, tokenizer, remove_stopwords=False):\n",
    "    # Function to split a request into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(tma.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call maintenance to get a list of words\n",
    "            sentences.append(maintenance( raw_sentence,\\\n",
    "              remove_stopwords))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n",
      "Wall time: 57 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "for tma in tontozona[\"ActionRequested\"]:\n",
    "    sentences += request_to_sentences(tma, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model\n",
    "With the list of parsed sentences, we're ready to train the model. There are a number of parameter choices that affect the run time and the quality of the final model that is produced. For details on the algorithms below, see the word2vec API documentation as well as the Google documentation.\n",
    "\n",
    "Architecture: Architecture options are skip-gram (default) or continuous bag of words. We found that skip-gram was very slightly slower but produced better results. Training algorithm: Hierarchical softmax (default) or negative sampling. For us, the default worked well. Downsampling of frequent words: \n",
    "\n",
    "The Google documentation recommends values between .00001 and .001. For us, values closer 0.001 seemed to improve the accuracy of the final model. Word vector dimensionality: More features result in longer runtimes, and often, but not always, result in better models. Reasonable values can be in the tens to hundreds; we used 300. Context / window size: How many words of context should the training algorithm take into account? \n",
    "\n",
    "10 seems to work well for hierarchical softmax (more is better, up to a point). Worker threads: Number of parallel processes to run. This is computer-specific, but between 4 and 6 should work on most systems. Minimum word count: This helps limit the size of the vocabulary to meaningful words. Any word that does not occur at least this many times across all documents is ignored. Reasonable values could be between 10 and 100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 10000    # Word vector dimensionality                      \n",
    "min_word_count = 10   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-12 15:01:11,132 : INFO : collecting all words and their counts\n",
      "2017-07-12 15:01:11,135 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-07-12 15:01:11,138 : INFO : collected 129 word types from a corpus of 896 raw words and 260 sentences\n",
      "2017-07-12 15:01:11,140 : INFO : Loading a fresh vocabulary\n",
      "2017-07-12 15:01:11,142 : INFO : min_count=10 retains 17 unique words (13% of original 129, drops 112)\n",
      "2017-07-12 15:01:11,144 : INFO : min_count=10 leaves 685 word corpus (76% of original 896, drops 211)\n",
      "2017-07-12 15:01:11,147 : INFO : deleting the raw counts dictionary of 129 items\n",
      "2017-07-12 15:01:11,154 : INFO : sample=0.001 downsamples 17 most-common words\n",
      "2017-07-12 15:01:11,156 : INFO : downsampling leaves estimated 96 word corpus (14.2% of prior 685)\n",
      "2017-07-12 15:01:11,158 : INFO : estimated required memory for 17 words and 10000 dimensions: 1368500 bytes\n",
      "2017-07-12 15:01:11,206 : INFO : resetting layer weights\n",
      "2017-07-12 15:01:11,284 : INFO : training model with 4 workers on 17 vocabulary and 10000 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-12 15:01:11,366 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-07-12 15:01:11,371 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-07-12 15:01:11,379 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-07-12 15:01:11,486 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-07-12 15:01:11,488 : INFO : training on 4480 raw words (427 effective words) took 0.2s, 2378 effective words/s\n",
      "2017-07-12 15:01:11,490 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 361 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('inspection', 0.02311844937503338),\n",
       " ('equipment', 0.013965419493615627),\n",
       " ('quarterly', 0.012161565013229847),\n",
       " ('handler', 0.011318184435367584),\n",
       " ('water', 0.010789703577756882),\n",
       " ('biannual', 0.010148197412490845),\n",
       " ('air', 0.006586567964404821),\n",
       " ('evaporative', 0.004941080696880817),\n",
       " ('dx', 0.004720899276435375),\n",
       " ('in', 0.0017127208411693573)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"fire\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
